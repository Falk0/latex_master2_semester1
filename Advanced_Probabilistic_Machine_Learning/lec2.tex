\section{Bayes' theorem}
We can find $p (\Theta|\mathcal{D})$ by using \textbf{Bayes' theorem}

\begin{equation}
    p (\theta|\mathcal{D}) = \frac{p (\mathcal{D}|\Theta)p(\Theta)} {p(\mathcal{D})} 
\end{equation}

We can view $p(\mathcal{D})$ as a normalized constant and if we view the quantities as functions
of $\Theta$ we can write:

\begin{equation}
    \underset{\text{posterior}}{p (\Theta|\mathcal{D})}  \propto \underset{\text{likelihood}}{p (\mathcal{D}|\Theta)} \underset{\text{prior}}{p(\Theta)}
\end{equation}  


$\propto$ means that it is proportional with respect to $\Theta$

\subsection*{Binomial-beta conjugate pair}
The binomial-beta conjugate pair refers to the Bayesian statistical framework where the binomial distribution, describing the likelihood of observed successes and failures, is combined with a beta distribution prior, resulting in a beta distribution posterior.

\begin{example}{Example: Flipping a damaged coin}
    \begin{itemize}
        \item x = 1 represents "head" and x = 0 represents "tail"
        \item The probability of x = 1 is:
            $p(x=1 | \mu) = \mu, \quad 0 \le \mu  \le 1$
    \end{itemize}
    we assume a damaged coin here, so it is not necessary 0.5
    
    \vspace{2em}

    \textbf{Question:} Given a dataset $\mathcal{D} = \{ x_1, \dots, x_n\}$, what is $p(\mu|\mathcal{D})$?

    \vspace*{1em}

    \textbf{Solution:} Bayes' theorem states that

        \begin{equation}
            p(\mu|\mathcal{D}) = \frac{p (\mathcal{D}|\mu)p(\mu)} {p(\mathcal{D})} 
        \end{equation}

        Find the likelihood and prior and then multiply! Lets start with the likelihood \\
        We know that :

        \begin{equation}
        \begin{aligned}
            p(x=1|\mu) = \mu \\
            p(x=1|\mu) = 1 - \mu
        \end{aligned}
        \end{equation}
        which is known as the \textcolor{red}{Bernoulli} distribution
% plug in x into the distribution 
        \begin{equation}
            p(x|\mu) = \text{Bern}(x;\mu) = \mu^{x}(1-\mu)^{1-x}
        \end{equation}

        The observations $x_1, \dots, x_N$ are \textcolor{red}{conditionally independent} given $\mu$ and the likelihood is then:

        \begin{equation}
        \begin{aligned}
            p(x_1, \dots, x_N | \mu) = \prod_{n=1}^{N} p(x_n|n) \\
            = \prod_{n=1}^{N} p(x_n|n) \mu^{x_n}(1-\mu)^{1 - x_n} \\
            = \mu^{m} (1-\mu)^{N-m}
        \end{aligned}
        \end{equation}
        where $m = \sum_{n=1}^{N}x_n$ i.e the number of heads. \textbf{Note}: the likelihood only depend on the data $\mathcal{D}$ via m. 
        Hence we can modify our problem slightly $p (\mu|m) \propto \underset{\text{likelihood}}{p (m|\mu)} \underset{\text{prior}}{p(\mu)}$

        Hence the likelihood is given by the \textcolor{red}{binomial distribution}

        \begin{equation}
            p(m|\mu) = \text{Bin}(m;N,\mu) = \begin{pmatrix} N \\ M \end{pmatrix} \mu^{m}(1-\mu)^{N-m}
        \end{equation}

        where $\begin{pmatrix} N \\ m \end{pmatrix} = \frac{N!} {(N-m)!m!} $ is the number of sequences giving $m$ heads. 

        \vspace{1em}

        Remember Bayes theorem $p(\mu|m) \propto p(m|\mu)p(\mu)$. There exists multiple possible prior distributions $p(\mu)$ and we opt for
        a prior which has attractive analytical properties. So we choose a prior such that the posterior will be of the same functional form as the prior. We call this a \textcolor{red}{conjugate prior}.
        The conjugate prior of the Binomial distribution is the \textcolor{red}{Beta distribution}:
        
       \begin{equation}
       \begin{aligned}
        \text{Beta}(\mu; a,b ) \propto \mu^{a-1} (1-\mu)^{b-1} \\
        = \frac{\Gamma (a+b)} {\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \\
        \text{where } \Gamma(a) = \int_{0}^{\infty} e^{-x}x^{a-1}\,\text{d}x  
       \end{aligned}
       \end{equation}
% Beta distribution fulfill def on 0-1 and its flexible
       The posterior can now be computed

       \begin{equation}
       \begin{aligned}
        p(\mu|m) \propto p(m|\mu)p(\mu) \\
        = \text{Bin} (m; N, \mu) \text{Beta}(\mu;a,b) \\
        \propto \mu^{m}(1-\mu)^{N-m}\mu^{a-1}(1-\mu)^{b-1} \\
        = \mu^{m+a-1}(1-\mu)^{N-m+b-1}
       \end{aligned}
       \end{equation}


    \end{example}

    \begin{example}{Example: cont. }
        Hence is the posterior also a Beta distribution:
 
        \begin{equation}
        \begin{aligned}
            p(\mu|m) = \text{Beta}(\mu; a^{\star},b^{\star}) \\
            \text{where} a^{\star} = m+1, b^{\star} = N-m+b 
        \end{aligned}
        \end{equation}
    
    \textbf{Bayesian inference:}
    \begin{itemize}
        \item Start with equal probability for all $\mu \Rightarrow \text{Beta}(\mu;1,1) =1$
        \item Assume that we get one data point $x_1 = 1$
        \item The posterior $\propto$ likelihood $\times$ prior
    \end{itemize} 

    Continue and assume we get new data points, $N = 5$ of which $m =4$ are heads, $\mathcal{D} = \{1,0,1,1,1\}$. \textcolor{red}{See slides, update later}

    \end{example}



    \subsection*{Real world applications}
    To give an idea of then this can be used, here are some real world applications:
    
    \begin{itemize}
        \item \textbf{Engineering}: Estimating the proportion of aircraft turbines blades that posses a structural defect after fabrication.
        \item \textbf{Social science}: Estimating the proportion of individuals who respond yes on a census question. 
        \item \textbf{Data science}: Estimating the proportion of individuals who click on an ad when visiting a website 
    \end{itemize}

    \section{Gauss-Gauss conjugate pair}
    The Gauss-Gauss conjugate pair refers to the Bayesian statistical framework where a Gaussian (normal) distribution likelihood, describing observed data, is combined with a Gaussian prior, resulting in a Gaussian posterior. \\
    So for a scalar random variable $X$:

    \begin{itemize}
        \item Expected value: \\
        \begin{equation}
            \mathbb{E}[X] = \int_{-\infty}^{\infty} xp(x)\,\text{d}x
        \end{equation}
        \item Variance: \\
        \begin{equation}
            \text{Var}[X] = \mathbb{E}[(X-\mu)^{2}]= \mathbb{E}[X^{2}] - \mu^{2}, \quad \text{where } \mu = \mathbb{E}[X] 
        \end{equation}
        \item Standard deviation: $\alpha = sqr^{\text{Var}[X]}$
    \end{itemize}

    For a scalar variable x. the Gaussian distribution can be written on the form:

    \begin{equation}
        \mathcal{N}(x; \hat{\mu}, \Sigma) =  \underset{Z}{\frac{1} {(2\pi)^{D/2} \sqrt{{\text{det}\Sigma}}}} \text{exp} \left( -\frac{1} {2} (x- \hat{\mu}^{T} \Sigma^{-1}(x-\hat{\mu}))  \right) 
    \end{equation}
    
    \begin{itemize}
        \item $\mathbb{E}[x] = \hat{\mu} $
        \item Cov$[x] = \Sigma$
        \item item $Z$ is the normalization constant
    \end{itemize}
    
    We let:
    \begin{equation}
    \begin{aligned}
        p(x) = \mathcal{N} (x; \mu, \sigma^{2}) \\
        p(y | x) = \mathcal{N} (y;x, \eta^{2})
    \end{aligned}
    \end{equation}

    We then have:

    \begin{equation}
    \begin{aligned}
        p(x|y) = \frac{p(y|x)p(x)} {p(y)} \\
        = \mathcal{N}(x; m, s^{2}) 
    \end{aligned}
    \end{equation}

    with the following:

    \begin{equation}
    \begin{aligned}
        s^{2} = \frac{1} {\sigma^{-2}+ \eta^{-2}} \\
        m = \frac{\sigma^{-2} + \eta^{-2}y} {\sigma^{-2} + \eta^{-2}}  
    \end{aligned}
    \end{equation}

    \subsection*{Expected value vector}
    For a vector random variable $\textbf{X} = \begin{bmatrix} X_1 \\  \vdots \\ X_N \end{bmatrix}$ we have:
    
    \begin{itemize}
        \item Expected value: \\
        \begin{equation}
            \begin{bmatrix} \mathcal{E}[X_1] \\ \vdots \\ \mathcal{E}[X_N] \end{bmatrix}
        \end{equation}
        \item The covariance matrix: \\
        \begin{equation}
            \text{Cov} [\bar{X}] = \mathcal{E}[(\bar{X}- \bar{\mu})(\bar{X}-\mu)^{T}] = \mathbb{E}[\textbf{XX}^{T}] - \bar{\mu}\bar{\mu}^{T}
        \end{equation}
    \end{itemize}

\subsection*{Multivariate Gaussian}
For a D-dimensional vector $x$, the \textcolor{red}{multivariate} Gaussian distribution can be written on the form:

\begin{equation}
    \mathcal{N} (x; \bar{\mu}, \Sigma ) = \mathcal{N}(x; \hat{\mu}, \Sigma) =  \underset{Z}{\frac{1} {(2\pi)^{D/2} \sqrt{{\text{det}\Sigma}}}} \text{exp} \left( -\frac{1} {2} \underset{\text{quadtratic form}}{(x- \hat{\mu})^{T} \Sigma^{-1}(x-\hat{\mu}))}  \right) 
\end{equation}

\begin{itemize}
    \item $\mathbb{E}[x] = \mu$
    \item Cov$[x] = \Sigma$
    \item $Z$ is the normalization constant 
\end{itemize}

The Gaussian in proportional to $e^{\text{quadtratic form}}$

\subsection*{Partitioned Gaussian}
Partition the Gaussian random vector $\bar{x} \sim \mathcal{N} (\bar{\mu}, \Sigma)$, where
$ \bar{x} \in \mathbb{R}^{n}$ into two sets of random variables $\bar{x_a} \in \mathbb{R}^{n_a}$ and $\bar{x_b} \in \mathbb{R}^{n_b}$,

\begin{equation}
    x = \begin{pmatrix} \bar{x_a} \\ \bar{x_b} \end{pmatrix}, \quad \bar{\mu} = \begin{pmatrix} \bar{\mu_a} \\ \bar{\mu_b}  \end{pmatrix}, \quad \Sigma = \begin{pmatrix} \Sigma_{aa}&  \Sigma_{ab}\\ \Sigma_{ba}& \Sigma_{bb} \end{pmatrix} 
\end{equation}

\newpage

\subsection*{Affine transformation of multivariate Gauss}

\begin{wbox}{Corollary 1 (Affine transformation - conditional)}
    If we assume that $\bar{x_a}$ as well as $\bar{x_b}$ conditioned on $\bar{x_a}$ are Gaussian distributed according to:

    \begin{equation}
    \begin{aligned}
        p(\bar{x_a}) = \mathcal{N}(\bar{x_a}; \bar{\mu_a}, \Sigma_a), \\
        p(\bar{x_a}|\bar{x_b}) = \mathcal{N}(\bar{x_b}; \textbf{A}\bar{x_a} + \textbf{b}, \Sigma_{b|a} )  
    \end{aligned}
    \end{equation}

    Then the conditional distribution of $\bar{x_a}$ given $\bar{x_b}$  is:

    \begin{equation}
        p(\bar{x_a}|\bar{x_b}) = \mathcal{N}(\bar{x_a}; \bar{\mu_{a|b}}, \Sigma_{a|b})
    \end{equation}

    Together with:

    \begin{equation}
    \begin{aligned}
        \bar{{\mu_{a|b}}} = \Sigma_{a|b} (\Sigma_a^{-1}\mu_{a} + \textbf{A}^{T} \Sigma^{-1}_{a|b}(\bar{x_b} - \textbf{b})) \\
        \Sigma_{a|b} = (\Sigma_a^{-1} + \textbf{A}^{T} \Sigma_{b|a}^{-1}\textbf{A})^{-1} 
    \end{aligned}
    \end{equation}
\end{wbox}

\begin{wbox}{Corollary 2 (Affine transformation - Marginalization)}
    If we assume that $\bar{x_a}$ as well as $\bar{x_b}$ conditioned on $\bar{x_a}$ are Gaussian distributed according to:

    \begin{equation}
    \begin{aligned}
        p(\bar{x_a}) = \mathcal{N}(\bar{x_a}; \bar{\mu_a}, \Sigma_a), \\
        p(\bar{x_a}|\bar{x_b}) = \mathcal{N}(\bar{x_b}; \textbf{A}\bar{x_a} + \textbf{b}, \Sigma_{b|a} )  
    \end{aligned}
    \end{equation}

    Then the marginal distribution of $\bar{x_b}$ is  then given by $\bar{x_b}$  is:

    \begin{equation}    
        p(\bar{x_b}|\bar{x_b}) = \mathcal{N}(\bar{x_b}; \bar{\mu_{b}}, \Sigma_{b})
    \end{equation}

    Together with:

    \begin{equation}
    \begin{aligned}
        \bar{{\mu_{b}}} = \textbf{A}\bar{\mu_a} + \textbf{b} \\
        \Sigma_{b} = (\Sigma_{b|a} + \textbf{A} \Sigma_{a}\textbf{A})^{T} 
    \end{aligned}
    \end{equation}
\end{wbox}


\subsection*{Gaussian inference (scalar example) from Corollary 1}
We let:

\begin{equation}
\begin{aligned}
    p(x) = \mathcal{N}(x;\mu,\sigma^{2}) \\
    p(y|x) = \mathcal{N}(y; x, \eta^{2})
\end{aligned}
\end{equation}

With:

\begin{equation}
\begin{aligned}
    \bm{x_a} = x, \quad \bm{\mu_a} = \mu, \quad \Sigma_a = \sigma^{2} \\
    \bm{x_b} = y, \quad \textbf{A} = 1, \quad b \textbf{b} = 0, \quad \Sigma_{b|a} = \eta^{2}
\end{aligned}
\end{equation}

Now with Corollary 1 this gives us:

\begin{equation}
    p(x|y) = \mathcal{N}(x;m,s^{2})
\end{equation}

where

\begin{equation}
\begin{aligned}
    s^{2} = \frac{1} {\sigma^{-2} + \eta^{-2}} \\
    m = \frac{\sigma^{-2}\mu 0 \eta^{-2}y} {\sigma^{-2} + \eta^{-2}}  
\end{aligned}
\end{equation}

\begin{wbox}{}
    \begin{equation}
        \begin{aligned}
            p(x|y) \propto p(y|x)p(x) \\
            \propto  e^{-\frac{-(y-x)^{2}} {2\eta^{2}} } e^{- \frac{(x-\mu)^{2}} {2 \sigma^{2}} } \\
            \text{complete the squares} \\
            \frac{y^{2} - 2x + x^{2}} {\eta^{2} } + \frac{x^{2}-2x\mu + \mu^{2}} {\sigma^{2}} \\
            = (\frac{1} {\eta^{2}} + \frac{1} {\sigma^{2}} ) x^{2} - 2(\frac{y} {\eta^{2}} + \frac{\mu} {\sigma^{2}} )x + \frac{y^{2}} {\eta^{2}} + \frac{\mu^{2}} {\sigma^{2}} \\
            = a (x- \frac{b} {a} )^{2} - \frac{b^{2}} {a} + c \\
            e^{\frac{1} {2}a(x - \frac{b} {a} )^{2} } e^{-\frac{b^{2}} {a} +c }
        \end{aligned}
        \end{equation}
\end{wbox}


\subsection*{Summarize lecture 2 with a few concepts}
\textcolor{red}{Conjugate prior} is a propr ensuring that the posterior and the prior belong to the same probability distribution family. \textcolor{red}{Bernoulli distribution} is a distribution for binary random variables. 
\textcolor{red}{Binomial distribution} is a distribution for the sum of multiple binary random variables.\textcolor{red}{Beta distribution}, the conjugate prior for the binomial distribution. \textcolor{red}{Gaussian distribution}, the well
known probability density function with the shape of the bell curve. \textcolor{red}{Multivariate Gaussian distribution} is a generalization of the Gaussian distribution to higher dimensions.

\begin{definition}{Theorem 1 - Marginalization}
Partition the Gaussian random vector $\bm{x} \sim \mathcal{N}(\bm{\mu}, \Sigma)$ according to:

 \begin{equation}
    \bm{x} = \begin{pmatrix} \bm{x}_a \\ \bm{x_b} \end{pmatrix}, \quad \bm{\mu} = \begin{pmatrix} \bm{\mu_a} \\ \bm{\mu_b} \end{pmatrix}, \quad \Sigma = \begin{pmatrix} \Sigma_{aa}& \Sigma_{ab} \\ \Sigma_{ba}& \Sigma_{bb} \end{pmatrix}
 \end{equation}

 The marginal distribution is then given by: $p(\bm{x}_a) = \mathcal{N}(\bm{x}_a ; \bm{\mu}_a, \Sigma_{aa})$ 

\end{definition}

\begin{definition}{Theorem 2: Conditioning }
    Partition the Gaussian random vector $\bm{x} \sim \mathcal{N}(\bm{\mu}, \Sigma)$ according to:

    \begin{equation}
        \bm{x} = \begin{pmatrix} \bm{x}_a \\ \bm{x_b} \end{pmatrix}, \quad \bm{\mu} = \begin{pmatrix} \bm{\mu_a} \\ \bm{\mu_b} \end{pmatrix}, \quad \Sigma = \begin{pmatrix} \Sigma_{aa}& \Sigma_{ab} \\ \Sigma_{ba}& \Sigma_{bb} \end{pmatrix}
     \end{equation}
    
The conditional distribution $p(\bm{x}_a|\bm{x}_b)$ is then given by:

\begin{equation}
\begin{aligned}
    p(\bm{x}_a|\bm{x}_b) = \mathcal{N}(\bm{x}_a;\bm{\mu}_{a|b}, \Sigma_{a|b}), \\
    \bm{\mu} = \bm{\mu}_a + \Sigma_{ab}\Sigma_{bb}^{-1}(\bm{x_b}-\bm{\mu}_b) \\
    \Sigma_{a|b} = \Sigma_{aa} - \Sigma_{ab} \Sigma_{bb}^{-1} \Sigma_{ba}
\end{aligned}
\end{equation}

\end{definition}

\begin{definition}{Theorem 3 - Affine transformation}
    Lets assume that $\textbf{x}_a$ as well as $\textbf{x}_b$ conditioned on $\textbf{x}_a$ are Gaussian distributed according to following:

    \begin{equation}
    \begin{aligned}
        p(\bm{x}_a) = \mathcal{N}(\bm{x}_a; \bm{\mu}, \Sigma_a), \\
        p(\bm{x}_b|\bm{x}_a) = \mathcal{N}(\bm{x}_b; \textbf{A}\bm{x}_a + \textbf{b}, \Sigma_{b|a})
    \end{aligned}
    \end{equation}

    The joint distribution of $\textbf{x}_a$ and $\textbf{x}_b$ is then:

    \begin{equation}
        p(\bm{x}_a|\bm{x}_b) = \mathcal{N} \left( \begin{bmatrix} \bm{x}_a \\ \bm{x}_b \end{bmatrix} ; \begin{bmatrix} \bm{\mu}_a \\ \textbf{A}\bm{\mu}_a + \textbf{b} \end{bmatrix}, \textbf{R} \right)
    \end{equation}

    with R equal to:

    \begin{equation}
        \textbf{R} = \begin{bmatrix} \Sigma_a & \Sigma_a \textbf{A}^{T} \\ \textbf{A}\Sigma_a & \Sigma_{b|a} + \textbf{A}\Sigma_a \textbf{A}^{T}  \end{bmatrix}
    \end{equation}

\end{definition}


 
% 
% 
